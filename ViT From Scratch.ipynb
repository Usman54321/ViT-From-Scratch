{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0UA6WFgcYYMI"
      },
      "outputs": [],
      "source": [
        "# import packages here\n",
        "import torch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "#       Model Training Function\n",
        "#--------------------------------------------------\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "def trainModel(net, trainloader, train_option, testloader=None):\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  lr = train_option['lr']\n",
        "  epoch = train_option['epoch']\n",
        "  device = train_option['device'] if 'device' in train_option.keys() else 'cpu'\n",
        "  log_iter = train_option['log_iter'] if 'log_iter' in train_option.keys() else 20\n",
        "  eval_epoch = 1\n",
        "\n",
        "  if 'optimizer' in train_option.keys():\n",
        "    optimizer = train_option['optimizer']\n",
        "  else:\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "  start_time = time.time()\n",
        "  if device == 'gpu':\n",
        "    net = net.cuda()\n",
        "\n",
        "  iters = 0\n",
        "  running_loss = 0.0\n",
        "  for ep in range(epoch):\n",
        "    net.train()\n",
        "    for iter, (x, y) in enumerate(trainloader):\n",
        "      iters += 1\n",
        "      batch_x = Variable(x).float()\n",
        "      batch_y = Variable(y).long()\n",
        "      if device == 'gpu':\n",
        "        batch_x = batch_x.cuda()\n",
        "        batch_y = batch_y.cuda()\n",
        "\n",
        "      outputs = net(batch_x)\n",
        "      loss = loss_func(outputs, batch_y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
        "      if iter % log_iter == 0:\n",
        "        print('Epoch:{:2d} | Iter:{:5d} | Time: {} | Train Loss: {:.4f} | Average Loss: {:.4f} '.format(ep+1, iter, time_lapse, loss.item(), running_loss/iters))\n",
        "\n",
        "    if testloader is not None and ep % eval_epoch == 0:\n",
        "      evalModel(net, testloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "#       Model Evaluating Function\n",
        "#--------------------------------------------------\n",
        "import time\n",
        "\n",
        "def evalModel(net, testloader):\n",
        "  acc = 0.0\n",
        "  count = 0\n",
        "  start_time = time.time()\n",
        "  device = 'gpu' if next(net.parameters()).is_cuda else 'cpu'\n",
        "  net.eval()\n",
        "\n",
        "  for iter, (x, y) in enumerate(testloader):\n",
        "        count += x.shape[0]\n",
        "        batch_x = Variable(x).float()\n",
        "        batch_y = Variable(y).long()\n",
        "        if device == 'gpu':\n",
        "          batch_x = batch_x.cuda()\n",
        "          batch_y = batch_y.cuda()\n",
        "        outputs = net(batch_x)\n",
        "        acc += torch.sum(outputs.max(1)[1]==batch_y)\n",
        "\n",
        "  time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n",
        "  print('Accuracy: {:5f} | Time: {}'.format(acc/count,time_lapse))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYLg1O6GnxOb"
      },
      "source": [
        "First I will build a Transformer Encoder Layer. Vison Transformers consist of multiple transformer encoder layers (i.e. the left block of the following image). My implementation includes a multi-head attention layer, a feed forward layer, two norm layers and two residual connection.\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/method_collections/trans.jpeg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FWXXZiYpUbH-"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dims = 128,\n",
        "               dropout=0.1,\n",
        "               mlp_hidden_dim = 32,\n",
        "               num_heads = 2,\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    # Multihead Attn\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=embedding_dims, num_heads=num_heads, dropout=dropout)\n",
        "\n",
        "    # Feedforward Layer\n",
        "    self.feed_forward = nn.Sequential(\n",
        "      nn.Linear(embedding_dims, mlp_hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(mlp_hidden_dim, embedding_dims),\n",
        "    )\n",
        "\n",
        "    # Layer Norm\n",
        "    self.norm1 = nn.LayerNorm(embedding_dims)\n",
        "    self.norm2 = nn.LayerNorm(embedding_dims)\n",
        "\n",
        "    # Dropout\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Multi-Head Self-Attention\n",
        "    attn_output, _ = self.attention(x, x, x)\n",
        "    x = x + self.dropout(attn_output)\n",
        "    x = self.norm1(x)\n",
        "\n",
        "    # Feedforward Layer\n",
        "    ff_output = self.feed_forward(x)\n",
        "    x = x + self.dropout(ff_output)\n",
        "    x = self.norm2(x)\n",
        "\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgAZ-9DQueBl"
      },
      "source": [
        "<img src=https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png width=\"400\">\n",
        "\n",
        "Now I will start coding the Vision Transformer. My ViT consists of:\n",
        "\n",
        "\n",
        "**1) Data Preprocessing**: Implementing the patchify process to transfer the image (batch_size, channel, height, width) to a sequence of tokens (batch_size, num_tokens, embedding_dimension).\n",
        "\n",
        "\n",
        "**2) Positional Encoding**\n",
        "\n",
        "**3) Extra Learnable [CLASS] embedding**\n",
        "\n",
        "**4) Transformer Encoder**: Constructing a transformer encoder by TransformerEncoderLayer you have already built.\n",
        "\n",
        "**5) Prediction**: Building the MLP Head and making classification.\n",
        "\n",
        "I will train my ViT on MNIST and\n",
        " - report the test accuracy\n",
        " - save my ViT model in a checkpoint file named: **./vit_trained.pt**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cRWlimcVUYgT"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=64,\n",
        "                 in_channels=1,\n",
        "                 patch_size=16,\n",
        "                 embedding_dims=128,\n",
        "                 num_transformer_layers=2,\n",
        "                 dropout=0.1,\n",
        "                 mlp_hidden_dim=128,\n",
        "                 num_heads=2,\n",
        "                 num_classes=22):\n",
        "        super().__init__()\n",
        "        '''\n",
        "    img_size: img height and width.\n",
        "    in_channels: 3 if RGB image, 1 if gray iamge\n",
        "    patch_size: the number of patches of an image is patch_size * patch_size\n",
        "    embedding_dims: feature dimension of tokens\n",
        "    num_transformer_layers: number of transformer encoder layer\n",
        "    dropout: probability of dropout\n",
        "    mlp_hidden_dim: hidden dim of MLP block\n",
        "    num_heads: number of heads in multi-head attention\n",
        "    num_classes: number of classes to predict\n",
        "\n",
        "    You are suggested but NOT required to use all inputs.\n",
        "    You MUST have num_transformer_layers and num_heads to control the\n",
        "    number of encoder layers, and number of head for multi-head attention.\n",
        "\n",
        "    Numbers given as inputs are default values, you can change them to get\n",
        "    a better accuracy.\n",
        "    '''\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embedding = nn.Conv2d(in_channels, embedding_dims, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Position Embedding\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches, embedding_dims))\n",
        "\n",
        "        # Class Embedding\n",
        "        self.class_embedding = nn.Parameter(torch.rand(1, embedding_dims))\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        self.transformer_encoder_layers = nn.ModuleList()\n",
        "        for _ in range(num_transformer_layers):\n",
        "            self.transformer_encoder_layers.append(TransformerEncoderLayer(\n",
        "                embedding_dims, dropout, mlp_hidden_dim, num_heads))\n",
        "\n",
        "        # Classification Head\n",
        "        _classification_head_shape = (self.num_patches + 1) * embedding_dims\n",
        "        self.classification_head = nn.Linear(_classification_head_shape, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # Reshape into sequence of tokens\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Position Embedding\n",
        "        x = x + self.position_embedding\n",
        "\n",
        "        # Class Embedding\n",
        "        x = torch.stack([torch.vstack((self.class_embedding, x[i])) for i in range(len(x))])\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        for layer in self.transformer_encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification Head\n",
        "        x = x.flatten(1)\n",
        "        x = self.classification_head(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# net = ViT()\n",
        "# first_x_batch = trainloader_small[0][0]\n",
        "# summary(net, input_size=first_x_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1eSSvfp8LZgR"
      },
      "outputs": [],
      "source": [
        "#load MNIST dataset\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = ToTensor()\n",
        "train_set = MNIST(root='./sample_data', train=True, download=True, transform=transform)\n",
        "test_set = MNIST(root='./sample_data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c7e_yEoHUum5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Iter:    0 | Time: 00:00:03 | Train Loss: 3.1802 | Average Loss: 3.1802 \n",
            "Epoch: 1 | Iter:   20 | Time: 00:00:04 | Train Loss: 1.0668 | Average Loss: 1.3398 \n",
            "Epoch: 1 | Iter:   40 | Time: 00:00:04 | Train Loss: 0.9968 | Average Loss: 1.1111 \n",
            "Epoch: 1 | Iter:   60 | Time: 00:00:05 | Train Loss: 0.8847 | Average Loss: 1.0144 \n",
            "Epoch: 1 | Iter:   80 | Time: 00:00:06 | Train Loss: 0.6256 | Average Loss: 0.9372 \n",
            "Epoch: 1 | Iter:  100 | Time: 00:00:07 | Train Loss: 0.6232 | Average Loss: 0.8811 \n",
            "Epoch: 1 | Iter:  120 | Time: 00:00:08 | Train Loss: 0.5890 | Average Loss: 0.8317 \n",
            "Epoch: 1 | Iter:  140 | Time: 00:00:09 | Train Loss: 0.6382 | Average Loss: 0.7966 \n",
            "Epoch: 1 | Iter:  160 | Time: 00:00:10 | Train Loss: 0.5434 | Average Loss: 0.7661 \n",
            "Epoch: 1 | Iter:  180 | Time: 00:00:11 | Train Loss: 0.7342 | Average Loss: 0.7407 \n",
            "Epoch: 1 | Iter:  200 | Time: 00:00:11 | Train Loss: 0.4170 | Average Loss: 0.7200 \n",
            "Epoch: 1 | Iter:  220 | Time: 00:00:12 | Train Loss: 0.6140 | Average Loss: 0.7037 \n",
            "Epoch: 1 | Iter:  240 | Time: 00:00:13 | Train Loss: 0.5148 | Average Loss: 0.6861 \n",
            "Epoch: 1 | Iter:  260 | Time: 00:00:14 | Train Loss: 0.4526 | Average Loss: 0.6690 \n",
            "Epoch: 1 | Iter:  280 | Time: 00:00:15 | Train Loss: 0.6157 | Average Loss: 0.6564 \n",
            "Epoch: 1 | Iter:  300 | Time: 00:00:16 | Train Loss: 0.4590 | Average Loss: 0.6477 \n",
            "Epoch: 1 | Iter:  320 | Time: 00:00:16 | Train Loss: 0.4810 | Average Loss: 0.6383 \n",
            "Epoch: 1 | Iter:  340 | Time: 00:00:17 | Train Loss: 0.5425 | Average Loss: 0.6294 \n",
            "Epoch: 1 | Iter:  360 | Time: 00:00:18 | Train Loss: 0.3212 | Average Loss: 0.6201 \n",
            "Epoch: 1 | Iter:  380 | Time: 00:00:19 | Train Loss: 0.4382 | Average Loss: 0.6108 \n",
            "Epoch: 1 | Iter:  400 | Time: 00:00:19 | Train Loss: 0.4433 | Average Loss: 0.6046 \n",
            "Epoch: 1 | Iter:  420 | Time: 00:00:20 | Train Loss: 0.5168 | Average Loss: 0.5973 \n",
            "Epoch: 1 | Iter:  440 | Time: 00:00:21 | Train Loss: 0.4695 | Average Loss: 0.5899 \n",
            "Epoch: 1 | Iter:  460 | Time: 00:00:22 | Train Loss: 0.3351 | Average Loss: 0.5823 \n",
            "Accuracy: 0.871200 | Time: 00:00:01\n",
            "Epoch: 2 | Iter:    0 | Time: 00:00:24 | Train Loss: 0.3314 | Average Loss: 0.5803 \n",
            "Epoch: 2 | Iter:   20 | Time: 00:00:24 | Train Loss: 0.5386 | Average Loss: 0.5751 \n",
            "Epoch: 2 | Iter:   40 | Time: 00:00:25 | Train Loss: 0.4203 | Average Loss: 0.5692 \n",
            "Epoch: 2 | Iter:   60 | Time: 00:00:26 | Train Loss: 0.4884 | Average Loss: 0.5637 \n",
            "Epoch: 2 | Iter:   80 | Time: 00:00:27 | Train Loss: 0.3873 | Average Loss: 0.5575 \n",
            "Epoch: 2 | Iter:  100 | Time: 00:00:27 | Train Loss: 0.4283 | Average Loss: 0.5518 \n",
            "Epoch: 2 | Iter:  120 | Time: 00:00:28 | Train Loss: 0.3158 | Average Loss: 0.5462 \n",
            "Epoch: 2 | Iter:  140 | Time: 00:00:29 | Train Loss: 0.2671 | Average Loss: 0.5406 \n",
            "Epoch: 2 | Iter:  160 | Time: 00:00:30 | Train Loss: 0.3665 | Average Loss: 0.5351 \n",
            "Epoch: 2 | Iter:  180 | Time: 00:00:30 | Train Loss: 0.5124 | Average Loss: 0.5294 \n",
            "Epoch: 2 | Iter:  200 | Time: 00:00:31 | Train Loss: 0.5055 | Average Loss: 0.5256 \n",
            "Epoch: 2 | Iter:  220 | Time: 00:00:32 | Train Loss: 0.3960 | Average Loss: 0.5220 \n",
            "Epoch: 2 | Iter:  240 | Time: 00:00:33 | Train Loss: 0.6053 | Average Loss: 0.5194 \n",
            "Epoch: 2 | Iter:  260 | Time: 00:00:34 | Train Loss: 0.3523 | Average Loss: 0.5151 \n",
            "Epoch: 2 | Iter:  280 | Time: 00:00:35 | Train Loss: 0.2663 | Average Loss: 0.5116 \n",
            "Epoch: 2 | Iter:  300 | Time: 00:00:36 | Train Loss: 0.4295 | Average Loss: 0.5077 \n",
            "Epoch: 2 | Iter:  320 | Time: 00:00:37 | Train Loss: 0.5463 | Average Loss: 0.5046 \n",
            "Epoch: 2 | Iter:  340 | Time: 00:00:38 | Train Loss: 0.5220 | Average Loss: 0.5017 \n",
            "Epoch: 2 | Iter:  360 | Time: 00:00:38 | Train Loss: 0.3699 | Average Loss: 0.4978 \n",
            "Epoch: 2 | Iter:  380 | Time: 00:00:39 | Train Loss: 0.4194 | Average Loss: 0.4945 \n",
            "Epoch: 2 | Iter:  400 | Time: 00:00:40 | Train Loss: 0.4607 | Average Loss: 0.4914 \n",
            "Epoch: 2 | Iter:  420 | Time: 00:00:41 | Train Loss: 0.4209 | Average Loss: 0.4880 \n",
            "Epoch: 2 | Iter:  440 | Time: 00:00:42 | Train Loss: 0.3982 | Average Loss: 0.4847 \n",
            "Epoch: 2 | Iter:  460 | Time: 00:00:43 | Train Loss: 0.3519 | Average Loss: 0.4829 \n",
            "Accuracy: 0.891200 | Time: 00:00:01\n",
            "Epoch: 3 | Iter:    0 | Time: 00:00:45 | Train Loss: 0.4371 | Average Loss: 0.4816 \n",
            "Epoch: 3 | Iter:   20 | Time: 00:00:45 | Train Loss: 0.2350 | Average Loss: 0.4787 \n",
            "Epoch: 3 | Iter:   40 | Time: 00:00:46 | Train Loss: 0.3356 | Average Loss: 0.4758 \n",
            "Epoch: 3 | Iter:   60 | Time: 00:00:47 | Train Loss: 0.2702 | Average Loss: 0.4732 \n",
            "Epoch: 3 | Iter:   80 | Time: 00:00:48 | Train Loss: 0.3299 | Average Loss: 0.4706 \n",
            "Epoch: 3 | Iter:  100 | Time: 00:00:48 | Train Loss: 0.2901 | Average Loss: 0.4680 \n",
            "Epoch: 3 | Iter:  120 | Time: 00:00:49 | Train Loss: 0.2892 | Average Loss: 0.4652 \n",
            "Epoch: 3 | Iter:  140 | Time: 00:00:50 | Train Loss: 0.3063 | Average Loss: 0.4626 \n",
            "Epoch: 3 | Iter:  160 | Time: 00:00:51 | Train Loss: 0.3982 | Average Loss: 0.4599 \n",
            "Epoch: 3 | Iter:  180 | Time: 00:00:51 | Train Loss: 0.3107 | Average Loss: 0.4576 \n",
            "Epoch: 3 | Iter:  200 | Time: 00:00:52 | Train Loss: 0.3146 | Average Loss: 0.4552 \n",
            "Epoch: 3 | Iter:  220 | Time: 00:00:53 | Train Loss: 0.2880 | Average Loss: 0.4531 \n",
            "Epoch: 3 | Iter:  240 | Time: 00:00:53 | Train Loss: 0.3944 | Average Loss: 0.4507 \n",
            "Epoch: 3 | Iter:  260 | Time: 00:00:54 | Train Loss: 0.3074 | Average Loss: 0.4480 \n",
            "Epoch: 3 | Iter:  280 | Time: 00:00:55 | Train Loss: 0.2794 | Average Loss: 0.4456 \n",
            "Epoch: 3 | Iter:  300 | Time: 00:00:56 | Train Loss: 0.4191 | Average Loss: 0.4439 \n",
            "Epoch: 3 | Iter:  320 | Time: 00:00:57 | Train Loss: 0.3820 | Average Loss: 0.4423 \n",
            "Epoch: 3 | Iter:  340 | Time: 00:00:58 | Train Loss: 0.3370 | Average Loss: 0.4403 \n",
            "Epoch: 3 | Iter:  360 | Time: 00:00:58 | Train Loss: 0.3333 | Average Loss: 0.4386 \n",
            "Epoch: 3 | Iter:  380 | Time: 00:00:59 | Train Loss: 0.3638 | Average Loss: 0.4367 \n",
            "Epoch: 3 | Iter:  400 | Time: 00:01:00 | Train Loss: 0.3805 | Average Loss: 0.4356 \n",
            "Epoch: 3 | Iter:  420 | Time: 00:01:01 | Train Loss: 0.2092 | Average Loss: 0.4344 \n",
            "Epoch: 3 | Iter:  440 | Time: 00:01:02 | Train Loss: 0.2933 | Average Loss: 0.4326 \n",
            "Epoch: 3 | Iter:  460 | Time: 00:01:03 | Train Loss: 0.4853 | Average Loss: 0.4312 \n",
            "Accuracy: 0.895900 | Time: 00:00:01\n",
            "Epoch: 4 | Iter:    0 | Time: 00:01:05 | Train Loss: 0.3213 | Average Loss: 0.4304 \n",
            "Epoch: 4 | Iter:   20 | Time: 00:01:06 | Train Loss: 0.3764 | Average Loss: 0.4286 \n",
            "Epoch: 4 | Iter:   40 | Time: 00:01:06 | Train Loss: 0.2990 | Average Loss: 0.4266 \n",
            "Epoch: 4 | Iter:   60 | Time: 00:01:07 | Train Loss: 0.3047 | Average Loss: 0.4252 \n",
            "Epoch: 4 | Iter:   80 | Time: 00:01:08 | Train Loss: 0.2859 | Average Loss: 0.4235 \n",
            "Epoch: 4 | Iter:  100 | Time: 00:01:09 | Train Loss: 0.2920 | Average Loss: 0.4215 \n",
            "Epoch: 4 | Iter:  120 | Time: 00:01:10 | Train Loss: 0.2463 | Average Loss: 0.4199 \n",
            "Epoch: 4 | Iter:  140 | Time: 00:01:10 | Train Loss: 0.4153 | Average Loss: 0.4183 \n",
            "Epoch: 4 | Iter:  160 | Time: 00:01:11 | Train Loss: 0.2465 | Average Loss: 0.4167 \n",
            "Epoch: 4 | Iter:  180 | Time: 00:01:12 | Train Loss: 0.2954 | Average Loss: 0.4151 \n",
            "Epoch: 4 | Iter:  200 | Time: 00:01:13 | Train Loss: 0.2939 | Average Loss: 0.4138 \n",
            "Epoch: 4 | Iter:  220 | Time: 00:01:13 | Train Loss: 0.1756 | Average Loss: 0.4120 \n",
            "Epoch: 4 | Iter:  240 | Time: 00:01:14 | Train Loss: 0.3936 | Average Loss: 0.4105 \n",
            "Epoch: 4 | Iter:  260 | Time: 00:01:15 | Train Loss: 0.3795 | Average Loss: 0.4094 \n",
            "Epoch: 4 | Iter:  280 | Time: 00:01:16 | Train Loss: 0.2477 | Average Loss: 0.4080 \n",
            "Epoch: 4 | Iter:  300 | Time: 00:01:16 | Train Loss: 0.2724 | Average Loss: 0.4069 \n",
            "Epoch: 4 | Iter:  320 | Time: 00:01:17 | Train Loss: 0.2634 | Average Loss: 0.4060 \n",
            "Epoch: 4 | Iter:  340 | Time: 00:01:18 | Train Loss: 0.2530 | Average Loss: 0.4048 \n",
            "Epoch: 4 | Iter:  360 | Time: 00:01:19 | Train Loss: 0.3490 | Average Loss: 0.4037 \n",
            "Epoch: 4 | Iter:  380 | Time: 00:01:19 | Train Loss: 0.4050 | Average Loss: 0.4029 \n",
            "Epoch: 4 | Iter:  400 | Time: 00:01:20 | Train Loss: 0.4040 | Average Loss: 0.4018 \n",
            "Epoch: 4 | Iter:  420 | Time: 00:01:21 | Train Loss: 0.2718 | Average Loss: 0.4005 \n",
            "Epoch: 4 | Iter:  440 | Time: 00:01:22 | Train Loss: 0.2990 | Average Loss: 0.3995 \n",
            "Epoch: 4 | Iter:  460 | Time: 00:01:22 | Train Loss: 0.2446 | Average Loss: 0.3984 \n",
            "Accuracy: 0.905500 | Time: 00:00:01\n",
            "Epoch: 5 | Iter:    0 | Time: 00:01:24 | Train Loss: 0.1953 | Average Loss: 0.3981 \n",
            "Epoch: 5 | Iter:   20 | Time: 00:01:25 | Train Loss: 0.1987 | Average Loss: 0.3965 \n",
            "Epoch: 5 | Iter:   40 | Time: 00:01:26 | Train Loss: 0.1986 | Average Loss: 0.3950 \n",
            "Epoch: 5 | Iter:   60 | Time: 00:01:27 | Train Loss: 0.2119 | Average Loss: 0.3936 \n",
            "Epoch: 5 | Iter:   80 | Time: 00:01:28 | Train Loss: 0.2017 | Average Loss: 0.3921 \n",
            "Epoch: 5 | Iter:  100 | Time: 00:01:29 | Train Loss: 0.3439 | Average Loss: 0.3909 \n",
            "Epoch: 5 | Iter:  120 | Time: 00:01:29 | Train Loss: 0.3990 | Average Loss: 0.3899 \n",
            "Epoch: 5 | Iter:  140 | Time: 00:01:30 | Train Loss: 0.4449 | Average Loss: 0.3890 \n",
            "Epoch: 5 | Iter:  160 | Time: 00:01:31 | Train Loss: 0.3089 | Average Loss: 0.3877 \n",
            "Epoch: 5 | Iter:  180 | Time: 00:01:32 | Train Loss: 0.1565 | Average Loss: 0.3868 \n",
            "Epoch: 5 | Iter:  200 | Time: 00:01:33 | Train Loss: 0.2798 | Average Loss: 0.3858 \n",
            "Epoch: 5 | Iter:  220 | Time: 00:01:34 | Train Loss: 0.2342 | Average Loss: 0.3848 \n",
            "Epoch: 5 | Iter:  240 | Time: 00:01:35 | Train Loss: 0.3952 | Average Loss: 0.3838 \n",
            "Epoch: 5 | Iter:  260 | Time: 00:01:36 | Train Loss: 0.1931 | Average Loss: 0.3828 \n",
            "Epoch: 5 | Iter:  280 | Time: 00:01:36 | Train Loss: 0.2118 | Average Loss: 0.3817 \n",
            "Epoch: 5 | Iter:  300 | Time: 00:01:37 | Train Loss: 0.3169 | Average Loss: 0.3807 \n",
            "Epoch: 5 | Iter:  320 | Time: 00:01:38 | Train Loss: 0.3036 | Average Loss: 0.3799 \n",
            "Epoch: 5 | Iter:  340 | Time: 00:01:39 | Train Loss: 0.1669 | Average Loss: 0.3792 \n",
            "Epoch: 5 | Iter:  360 | Time: 00:01:40 | Train Loss: 0.2106 | Average Loss: 0.3784 \n",
            "Epoch: 5 | Iter:  380 | Time: 00:01:40 | Train Loss: 0.1864 | Average Loss: 0.3774 \n",
            "Epoch: 5 | Iter:  400 | Time: 00:01:41 | Train Loss: 0.3676 | Average Loss: 0.3766 \n",
            "Epoch: 5 | Iter:  420 | Time: 00:01:42 | Train Loss: 0.2098 | Average Loss: 0.3759 \n",
            "Epoch: 5 | Iter:  440 | Time: 00:01:42 | Train Loss: 0.4123 | Average Loss: 0.3752 \n",
            "Epoch: 5 | Iter:  460 | Time: 00:01:43 | Train Loss: 0.2942 | Average Loss: 0.3747 \n",
            "Accuracy: 0.908700 | Time: 00:00:01\n",
            "Epoch: 6 | Iter:    0 | Time: 00:01:45 | Train Loss: 0.2222 | Average Loss: 0.3743 \n",
            "Epoch: 6 | Iter:   20 | Time: 00:01:46 | Train Loss: 0.2150 | Average Loss: 0.3731 \n",
            "Epoch: 6 | Iter:   40 | Time: 00:01:46 | Train Loss: 0.2325 | Average Loss: 0.3718 \n",
            "Epoch: 6 | Iter:   60 | Time: 00:01:47 | Train Loss: 0.2786 | Average Loss: 0.3709 \n",
            "Epoch: 6 | Iter:   80 | Time: 00:01:48 | Train Loss: 0.2543 | Average Loss: 0.3697 \n",
            "Epoch: 6 | Iter:  100 | Time: 00:01:49 | Train Loss: 0.2117 | Average Loss: 0.3688 \n",
            "Epoch: 6 | Iter:  120 | Time: 00:01:49 | Train Loss: 0.2621 | Average Loss: 0.3680 \n",
            "Epoch: 6 | Iter:  140 | Time: 00:01:50 | Train Loss: 0.1443 | Average Loss: 0.3671 \n",
            "Epoch: 6 | Iter:  160 | Time: 00:01:51 | Train Loss: 0.2674 | Average Loss: 0.3663 \n",
            "Epoch: 6 | Iter:  180 | Time: 00:01:52 | Train Loss: 0.1360 | Average Loss: 0.3654 \n",
            "Epoch: 6 | Iter:  200 | Time: 00:01:53 | Train Loss: 0.2660 | Average Loss: 0.3645 \n",
            "Epoch: 6 | Iter:  220 | Time: 00:01:54 | Train Loss: 0.2903 | Average Loss: 0.3636 \n",
            "Epoch: 6 | Iter:  240 | Time: 00:01:55 | Train Loss: 0.4126 | Average Loss: 0.3629 \n",
            "Epoch: 6 | Iter:  260 | Time: 00:01:55 | Train Loss: 0.3787 | Average Loss: 0.3625 \n",
            "Epoch: 6 | Iter:  280 | Time: 00:01:56 | Train Loss: 0.3732 | Average Loss: 0.3619 \n",
            "Epoch: 6 | Iter:  300 | Time: 00:01:57 | Train Loss: 0.2147 | Average Loss: 0.3611 \n",
            "Epoch: 6 | Iter:  320 | Time: 00:01:58 | Train Loss: 0.3562 | Average Loss: 0.3604 \n",
            "Epoch: 6 | Iter:  340 | Time: 00:01:59 | Train Loss: 0.2914 | Average Loss: 0.3598 \n",
            "Epoch: 6 | Iter:  360 | Time: 00:02:00 | Train Loss: 0.2729 | Average Loss: 0.3591 \n",
            "Epoch: 6 | Iter:  380 | Time: 00:02:00 | Train Loss: 0.2988 | Average Loss: 0.3585 \n",
            "Epoch: 6 | Iter:  400 | Time: 00:02:01 | Train Loss: 0.3642 | Average Loss: 0.3578 \n",
            "Epoch: 6 | Iter:  420 | Time: 00:02:02 | Train Loss: 0.2337 | Average Loss: 0.3570 \n",
            "Epoch: 6 | Iter:  440 | Time: 00:02:03 | Train Loss: 0.4716 | Average Loss: 0.3564 \n",
            "Epoch: 6 | Iter:  460 | Time: 00:02:03 | Train Loss: 0.1812 | Average Loss: 0.3557 \n",
            "Accuracy: 0.909500 | Time: 00:00:01\n",
            "Epoch: 7 | Iter:    0 | Time: 00:02:05 | Train Loss: 0.1977 | Average Loss: 0.3554 \n",
            "Epoch: 7 | Iter:   20 | Time: 00:02:06 | Train Loss: 0.1572 | Average Loss: 0.3545 \n",
            "Epoch: 7 | Iter:   40 | Time: 00:02:07 | Train Loss: 0.3409 | Average Loss: 0.3540 \n",
            "Epoch: 7 | Iter:   60 | Time: 00:02:07 | Train Loss: 0.3273 | Average Loss: 0.3534 \n",
            "Epoch: 7 | Iter:   80 | Time: 00:02:08 | Train Loss: 0.3734 | Average Loss: 0.3526 \n",
            "Epoch: 7 | Iter:  100 | Time: 00:02:09 | Train Loss: 0.3270 | Average Loss: 0.3518 \n",
            "Epoch: 7 | Iter:  120 | Time: 00:02:09 | Train Loss: 0.2350 | Average Loss: 0.3510 \n",
            "Epoch: 7 | Iter:  140 | Time: 00:02:10 | Train Loss: 0.3304 | Average Loss: 0.3504 \n",
            "Epoch: 7 | Iter:  160 | Time: 00:02:11 | Train Loss: 0.2308 | Average Loss: 0.3497 \n",
            "Epoch: 7 | Iter:  180 | Time: 00:02:12 | Train Loss: 0.2118 | Average Loss: 0.3491 \n",
            "Epoch: 7 | Iter:  200 | Time: 00:02:12 | Train Loss: 0.2228 | Average Loss: 0.3484 \n",
            "Epoch: 7 | Iter:  220 | Time: 00:02:13 | Train Loss: 0.2597 | Average Loss: 0.3478 \n",
            "Epoch: 7 | Iter:  240 | Time: 00:02:14 | Train Loss: 0.3359 | Average Loss: 0.3473 \n",
            "Epoch: 7 | Iter:  260 | Time: 00:02:15 | Train Loss: 0.3000 | Average Loss: 0.3467 \n",
            "Epoch: 7 | Iter:  280 | Time: 00:02:15 | Train Loss: 0.2959 | Average Loss: 0.3460 \n",
            "Epoch: 7 | Iter:  300 | Time: 00:02:16 | Train Loss: 0.3254 | Average Loss: 0.3454 \n",
            "Epoch: 7 | Iter:  320 | Time: 00:02:17 | Train Loss: 0.3408 | Average Loss: 0.3448 \n",
            "Epoch: 7 | Iter:  340 | Time: 00:02:18 | Train Loss: 0.1992 | Average Loss: 0.3442 \n",
            "Epoch: 7 | Iter:  360 | Time: 00:02:19 | Train Loss: 0.3213 | Average Loss: 0.3437 \n",
            "Epoch: 7 | Iter:  380 | Time: 00:02:20 | Train Loss: 0.1520 | Average Loss: 0.3432 \n",
            "Epoch: 7 | Iter:  400 | Time: 00:02:20 | Train Loss: 0.3437 | Average Loss: 0.3425 \n",
            "Epoch: 7 | Iter:  420 | Time: 00:02:21 | Train Loss: 0.1779 | Average Loss: 0.3419 \n",
            "Epoch: 7 | Iter:  440 | Time: 00:02:22 | Train Loss: 0.2492 | Average Loss: 0.3414 \n",
            "Epoch: 7 | Iter:  460 | Time: 00:02:23 | Train Loss: 0.2981 | Average Loss: 0.3407 \n",
            "Accuracy: 0.906300 | Time: 00:00:01\n",
            "Epoch: 8 | Iter:    0 | Time: 00:02:25 | Train Loss: 0.2875 | Average Loss: 0.3403 \n",
            "Epoch: 8 | Iter:   20 | Time: 00:02:26 | Train Loss: 0.2209 | Average Loss: 0.3396 \n",
            "Epoch: 8 | Iter:   40 | Time: 00:02:26 | Train Loss: 0.1810 | Average Loss: 0.3390 \n",
            "Epoch: 8 | Iter:   60 | Time: 00:02:27 | Train Loss: 0.1949 | Average Loss: 0.3384 \n",
            "Epoch: 8 | Iter:   80 | Time: 00:02:28 | Train Loss: 0.4156 | Average Loss: 0.3378 \n",
            "Epoch: 8 | Iter:  100 | Time: 00:02:29 | Train Loss: 0.2019 | Average Loss: 0.3373 \n",
            "Epoch: 8 | Iter:  120 | Time: 00:02:30 | Train Loss: 0.2331 | Average Loss: 0.3367 \n",
            "Epoch: 8 | Iter:  140 | Time: 00:02:31 | Train Loss: 0.1960 | Average Loss: 0.3360 \n",
            "Epoch: 8 | Iter:  160 | Time: 00:02:32 | Train Loss: 0.1760 | Average Loss: 0.3355 \n",
            "Epoch: 8 | Iter:  180 | Time: 00:02:32 | Train Loss: 0.3388 | Average Loss: 0.3349 \n",
            "Epoch: 8 | Iter:  200 | Time: 00:02:33 | Train Loss: 0.3644 | Average Loss: 0.3344 \n",
            "Epoch: 8 | Iter:  220 | Time: 00:02:34 | Train Loss: 0.2421 | Average Loss: 0.3337 \n",
            "Epoch: 8 | Iter:  240 | Time: 00:02:35 | Train Loss: 0.3086 | Average Loss: 0.3332 \n",
            "Epoch: 8 | Iter:  260 | Time: 00:02:35 | Train Loss: 0.1891 | Average Loss: 0.3326 \n",
            "Epoch: 8 | Iter:  280 | Time: 00:02:36 | Train Loss: 0.2737 | Average Loss: 0.3321 \n",
            "Epoch: 8 | Iter:  300 | Time: 00:02:37 | Train Loss: 0.2264 | Average Loss: 0.3317 \n",
            "Epoch: 8 | Iter:  320 | Time: 00:02:38 | Train Loss: 0.2491 | Average Loss: 0.3312 \n",
            "Epoch: 8 | Iter:  340 | Time: 00:02:38 | Train Loss: 0.1737 | Average Loss: 0.3306 \n",
            "Epoch: 8 | Iter:  360 | Time: 00:02:39 | Train Loss: 0.1965 | Average Loss: 0.3301 \n",
            "Epoch: 8 | Iter:  380 | Time: 00:02:40 | Train Loss: 0.2890 | Average Loss: 0.3297 \n",
            "Epoch: 8 | Iter:  400 | Time: 00:02:40 | Train Loss: 0.2526 | Average Loss: 0.3292 \n",
            "Epoch: 8 | Iter:  420 | Time: 00:02:41 | Train Loss: 0.2945 | Average Loss: 0.3288 \n",
            "Epoch: 8 | Iter:  440 | Time: 00:02:42 | Train Loss: 0.2850 | Average Loss: 0.3283 \n",
            "Epoch: 8 | Iter:  460 | Time: 00:02:43 | Train Loss: 0.2065 | Average Loss: 0.3279 \n",
            "Accuracy: 0.907800 | Time: 00:00:01\n",
            "Epoch: 9 | Iter:    0 | Time: 00:02:44 | Train Loss: 0.1417 | Average Loss: 0.3276 \n",
            "Epoch: 9 | Iter:   20 | Time: 00:02:45 | Train Loss: 0.1369 | Average Loss: 0.3270 \n",
            "Epoch: 9 | Iter:   40 | Time: 00:02:46 | Train Loss: 0.3383 | Average Loss: 0.3263 \n",
            "Epoch: 9 | Iter:   60 | Time: 00:02:47 | Train Loss: 0.1820 | Average Loss: 0.3258 \n",
            "Epoch: 9 | Iter:   80 | Time: 00:02:48 | Train Loss: 0.1777 | Average Loss: 0.3254 \n",
            "Epoch: 9 | Iter:  100 | Time: 00:02:49 | Train Loss: 0.1595 | Average Loss: 0.3248 \n",
            "Epoch: 9 | Iter:  120 | Time: 00:02:50 | Train Loss: 0.1762 | Average Loss: 0.3243 \n",
            "Epoch: 9 | Iter:  140 | Time: 00:02:50 | Train Loss: 0.3041 | Average Loss: 0.3238 \n",
            "Epoch: 9 | Iter:  160 | Time: 00:02:51 | Train Loss: 0.2142 | Average Loss: 0.3233 \n",
            "Epoch: 9 | Iter:  180 | Time: 00:02:52 | Train Loss: 0.2554 | Average Loss: 0.3228 \n",
            "Epoch: 9 | Iter:  200 | Time: 00:02:53 | Train Loss: 0.2745 | Average Loss: 0.3224 \n",
            "Epoch: 9 | Iter:  220 | Time: 00:02:54 | Train Loss: 0.1121 | Average Loss: 0.3219 \n",
            "Epoch: 9 | Iter:  240 | Time: 00:02:55 | Train Loss: 0.2934 | Average Loss: 0.3214 \n",
            "Epoch: 9 | Iter:  260 | Time: 00:02:56 | Train Loss: 0.2330 | Average Loss: 0.3211 \n",
            "Epoch: 9 | Iter:  280 | Time: 00:02:56 | Train Loss: 0.1830 | Average Loss: 0.3207 \n",
            "Epoch: 9 | Iter:  300 | Time: 00:02:57 | Train Loss: 0.2048 | Average Loss: 0.3203 \n",
            "Epoch: 9 | Iter:  320 | Time: 00:02:58 | Train Loss: 0.2562 | Average Loss: 0.3200 \n",
            "Epoch: 9 | Iter:  340 | Time: 00:02:59 | Train Loss: 0.1899 | Average Loss: 0.3195 \n",
            "Epoch: 9 | Iter:  360 | Time: 00:02:59 | Train Loss: 0.2366 | Average Loss: 0.3191 \n",
            "Epoch: 9 | Iter:  380 | Time: 00:03:00 | Train Loss: 0.2618 | Average Loss: 0.3187 \n",
            "Epoch: 9 | Iter:  400 | Time: 00:03:01 | Train Loss: 0.2612 | Average Loss: 0.3184 \n",
            "Epoch: 9 | Iter:  420 | Time: 00:03:02 | Train Loss: 0.1626 | Average Loss: 0.3180 \n",
            "Epoch: 9 | Iter:  440 | Time: 00:03:02 | Train Loss: 0.1926 | Average Loss: 0.3175 \n",
            "Epoch: 9 | Iter:  460 | Time: 00:03:03 | Train Loss: 0.2696 | Average Loss: 0.3171 \n",
            "Accuracy: 0.908000 | Time: 00:00:01\n",
            "Epoch:10 | Iter:    0 | Time: 00:03:05 | Train Loss: 0.1994 | Average Loss: 0.3169 \n",
            "Epoch:10 | Iter:   20 | Time: 00:03:05 | Train Loss: 0.2049 | Average Loss: 0.3164 \n",
            "Epoch:10 | Iter:   40 | Time: 00:03:06 | Train Loss: 0.2432 | Average Loss: 0.3160 \n",
            "Epoch:10 | Iter:   60 | Time: 00:03:07 | Train Loss: 0.2482 | Average Loss: 0.3156 \n",
            "Epoch:10 | Iter:   80 | Time: 00:03:08 | Train Loss: 0.1763 | Average Loss: 0.3152 \n",
            "Epoch:10 | Iter:  100 | Time: 00:03:09 | Train Loss: 0.1452 | Average Loss: 0.3147 \n",
            "Epoch:10 | Iter:  120 | Time: 00:03:09 | Train Loss: 0.1833 | Average Loss: 0.3141 \n",
            "Epoch:10 | Iter:  140 | Time: 00:03:10 | Train Loss: 0.3073 | Average Loss: 0.3135 \n",
            "Epoch:10 | Iter:  160 | Time: 00:03:11 | Train Loss: 0.2932 | Average Loss: 0.3132 \n",
            "Epoch:10 | Iter:  180 | Time: 00:03:12 | Train Loss: 0.2925 | Average Loss: 0.3128 \n",
            "Epoch:10 | Iter:  200 | Time: 00:03:13 | Train Loss: 0.2622 | Average Loss: 0.3124 \n",
            "Epoch:10 | Iter:  220 | Time: 00:03:13 | Train Loss: 0.1421 | Average Loss: 0.3120 \n",
            "Epoch:10 | Iter:  240 | Time: 00:03:14 | Train Loss: 0.2566 | Average Loss: 0.3116 \n",
            "Epoch:10 | Iter:  260 | Time: 00:03:15 | Train Loss: 0.2385 | Average Loss: 0.3112 \n",
            "Epoch:10 | Iter:  280 | Time: 00:03:16 | Train Loss: 0.2747 | Average Loss: 0.3107 \n",
            "Epoch:10 | Iter:  300 | Time: 00:03:17 | Train Loss: 0.2318 | Average Loss: 0.3102 \n",
            "Epoch:10 | Iter:  320 | Time: 00:03:17 | Train Loss: 0.2143 | Average Loss: 0.3098 \n",
            "Epoch:10 | Iter:  340 | Time: 00:03:18 | Train Loss: 0.3487 | Average Loss: 0.3094 \n",
            "Epoch:10 | Iter:  360 | Time: 00:03:19 | Train Loss: 0.2465 | Average Loss: 0.3089 \n",
            "Epoch:10 | Iter:  380 | Time: 00:03:20 | Train Loss: 0.1357 | Average Loss: 0.3086 \n",
            "Epoch:10 | Iter:  400 | Time: 00:03:21 | Train Loss: 0.2979 | Average Loss: 0.3084 \n",
            "Epoch:10 | Iter:  420 | Time: 00:03:22 | Train Loss: 0.2421 | Average Loss: 0.3079 \n",
            "Epoch:10 | Iter:  440 | Time: 00:03:22 | Train Loss: 0.1847 | Average Loss: 0.3076 \n",
            "Epoch:10 | Iter:  460 | Time: 00:03:23 | Train Loss: 0.2130 | Average Loss: 0.3073 \n",
            "Accuracy: 0.909100 | Time: 00:00:01\n"
          ]
        }
      ],
      "source": [
        "#--------------------------------------------------\n",
        "#       Start Training & Evaluation\n",
        "#--------------------------------------------------\n",
        "\n",
        "# define training options\n",
        "train_option = {}\n",
        "train_option['lr'] = 0.001\n",
        "train_option['epoch'] = 10\n",
        "train_option['device'] = 'gpu'\n",
        "\n",
        "# start training\n",
        "net = ViT()\n",
        "trainModel(net, train_loader, train_option, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy on Test Set: 0.909100 at epoch 10 | Time for training 00:03:22 | Time for testing 00:00:01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model as vit_trained.pt\n",
        "# torch.save(net.state_dict(), 'vit_trained.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYJOi8QYYYNG"
      },
      "source": [
        "<!--Write your report here in markdown or html-->\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
